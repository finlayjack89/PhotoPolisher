Phase 1: Fix Server-Side Bugs (AI Prompt & Shadow)
We will first improve the server's logic.

üêû Bug 1: AI Floor Detection
Fix: We will give the Gemini AI a much more detailed "chain of thought" prompt to force it to find the intersection of the floor and wall, which is what you're expecting.

File: server/image-processing/analyze-images.ts

TypeScript

// ... (imports) ...

export async function analyzeImages(req: AnalyzeImagesRequest) {
  // ... (this function is unchanged) ...
}

// THIS FUNCTION IS REPLACED
export async function analyzeBackdrop(req: AnalyzeImagesRequest) {
  const { files } = req;
  const file = files[0];

  const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
  const model = genAI.getGenerativeModel({ model: "gemini-pro-vision" });

  const imagePart = {
    inlineData: {
      data: file.data,
      mimeType: file.type,
    },
  };
  
  // --- THIS IS THE NEW, MORE ACCURATE PROMPT ---
  const prompt = `
    Analyze this product backdrop image. Your goal is to find the exact "floor line".
    Follow these steps:
    1. Identify the primary vertical surface (the "wall").
    2. Identify the primary horizontal surface (the "floor").
    3. Find the line where these two surfaces intersect.
    4. Return a JSON object with one key, "floorY", representing the vertical Y-coordinate of this intersection line as a fraction from 0.0 (top) to 1.0 (bottom).

    If you cannot find a clear wall/floor intersection, find the bottom-most horizontal surface and return that Y-coordinate.

    Example response for a typical studio shot:
    { "floorY": 0.78 }
  `;

  const result = await model.generateContent([prompt, imagePart]);
  
  let floorY = 0.75; // Default fallback
  try {
    const text = result.response.text();
    const jsonMatch = text.match(/\{.*\}/s);
    if (jsonMatch) {
      const parsed = JSON.parse(jsonMatch[0]);
      if (parsed.floorY && typeof parsed.floorY === 'number') {
        floorY = parsed.floorY;
      }
    }
  } catch (e) {
    console.error("Failed to parse floorY from Gemini:", e);
  }
  
  return { success: true, floorY };
}
üêû Bug 5: Poor Shadow Quality
Fix: We will update the Cloudinary API call to use advanced transformations for a softer, more realistic shadow.

File: server/image-processing/add-drop-shadow.ts

TypeScript

// ... (imports) ...

export async function addDropShadow(req: AddDropShadowRequest) {
  // ... (setup) ...

  const uploadPromises = req.images.map(img => {
    return new Promise((resolve, reject) => {
      // ... (base64 data strip) ...
      
      // --- THIS IS THE NEW, HIGHER-QUALITY SHADOW ---
      const uploadOptions = {
        public_id: `shadow_${img.name.split('.')[0]}`,
        overwrite: true,
        transformation: [
          {
            // Layer 1: A soft, blurred shadow
            effect: "shadow:50", // 50% opacity
            color: "#444444",
            x: 5,
            y: 10,
          },
          {
            // Layer 2: A sharper, darker contact shadow
            effect: "shadow:40",
            color: "#000000",
            x: 5,
            y: 10,
            radius: 5 // Tighter radius
          },
          { fetch_format: "png" }
        ]
      };
      
      // ... (rest of the function: stream, error handling) ...
    });
  });

  return Promise.all(uploadPromises)
    // ... (rest of the function) ...
}
Phase 2: Fix the Core Workflow (The Main Bug)
This is the most important fix. We will replace the old, buggy multi-step workflow with the new "Batch Processing" architecture.

File: src/components/CommercialEditingWorkflow.tsx

TypeScript

import React, { useState } from 'react';
import { BackdropPositioning } from './BackdropPositioning';
import { GalleryPreview } from './GalleryPreview';
import { BackgroundRemovalStep } from './BackgroundRemovalStep';
import { ImagePreviewStep } from './ImagePreviewStep';
import { SubjectPlacement } from "@/lib/canvas-utils";
import { useToast } from "@/hooks/use-toast";
// NEW: Import the new batch processor
import { BatchProcessingStep } from './BatchProcessingStep'; 

interface CommercialEditingWorkflowProps {
  files: (File & { isPreCut?: boolean; originalSize?: number })[];
  onBack: () => void;
}

// --- THIS IS THE NEW, CORRECTED WORKFLOW ---
type WorkflowStep = 
  | 'analysis' 
  | 'background-removal' 
  | 'positioning' 
  | 'batch-processing' // The new "waiting room"
  | 'complete' 
  | 'precut-positioning';

interface ProcessedImages {
  backgroundRemoved: Array<{ 
    name: string; 
    originalData: string; 
    backgroundRemovedData: string; 
    size: number; 
    originalSize?: number; 
  }>;
  finalComposited?: Array<{ name: string; compositedData: string; }>;
}

interface MasterRules {
  placement: SubjectPlacement;
  padding: number;
  aspectRatio: string;
}

// Helper types that were missing
type FileWithOriginalSize = File & {
  isPreCut?: boolean;
  originalSize: number;
};

type AnalysisResult = {
  originalName: string;
  originalSize: number;
  compressedSize: number;
  compressionRatio: string;
  qualityPercentage: number;
};

export const CommercialEditingWorkflow: React.FC<CommercialEditingWorkflowProps> = ({
  files,
  onBack
}) => {
  // --- FIX FOR THE CRASH ---
  // Initialize state directly from props to prevent "undefined" error
  const [workflowFiles, setWorkflowFiles] = useState<FileWithOriginalSize[]>(
    files.map(f => ({ ...f, name: f.name, size: f.size, type: f.type, originalSize: f.size, isPreCut: f.isPreCut }))
  );
  
  const [analysisResults, setAnalysisResults] = useState<AnalysisResult[]>(
    files.map(f => ({
      originalName: f.name,
      originalSize: (f as FileWithOriginalSize).originalSize || f.size,
      compressedSize: f.size,
      compressionRatio: (f as FileWithOriginalSize).originalSize ? `${Math.round(100 - (f.size / (f as FileWithOriginalSize).originalSize!) * 100)}%` : '0%',
      qualityPercentage: 92 
    }))
  );
  // --- END CRASH FIX ---

  const [currentStep, setCurrentStep] = useState<WorkflowStep>('analysis');
  const [processedImages, setProcessedImages] = useState<ProcessedImages>({ backgroundRemoved: [] });
  const [masterRules, setMasterRules] = useState<MasterRules | null>(null);
  const [backdrop, setBackdrop] = useState<string>("");
  const { toast } = useToast();

  const handleAnalysisComplete = () => {
    const allPreCut = workflowFiles.every(file => file.isPreCut);
    if (allPreCut) {
      setCurrentStep('precut-positioning');
    } else {
      setCurrentStep('background-removal');
    }
  };

  const handleBackgroundRemovalComplete = (subjects: any[]) => {
    console.log("Background removal complete. Received subjects:", subjects);
    setProcessedImages({ backgroundRemoved: subjects });
    setCurrentStep('positioning'); 
  };
  
  const handlePositioningComplete = (
    backdrop: string, 
    placement: SubjectPlacement, 
    padding: number,
    aspectRatio: string
  ) => {
    setBackdrop(backdrop);
    setMasterRules({ placement, padding, aspectRatio });
    // --- TRANSITION TO THE NEW BATCH STEP ---
    setCurrentStep('batch-processing');
  };

  const handleBatchProcessingComplete = (results: Array<{ name: string; compositedData: string }>) => {
    setProcessedImages(prev => ({
      ...prev,
      finalComposited: results
    }));
    setCurrentStep('complete');
  };

  if (currentStep === 'analysis') {
    return (
      <ImagePreviewStep
        files={workflowFiles}
        onContinue={handleAnalysisComplete}
        onBack={onBack}
        wasCompressed={true}
        compressionData={analysisResults}
      />
    );
  }

  if (currentStep === 'background-removal') {
    return (
      <BackgroundRemovalStep
        files={workflowFiles}
        onProcessingComplete={handleBackgroundRemovalComplete} 
        onContinue={handleBackgroundRemovalComplete}
        onBack={() => setCurrentStep('analysis')}
      />
    );
  }

  if (currentStep === 'positioning' || currentStep === 'precut-positioning') {
    const isPreCut = currentStep === 'precut-positioning';
    return (
      <BackdropPositioning
        allSubjects={isPreCut ? workflowFiles : processedImages.backgroundRemoved}
        isPreCut={isPreCut}
        onPositioningComplete={handlePositioningComplete}
        onBack={() => isPreCut ? setCurrentStep('analysis') : setCurrentStep('background-removal')}
      />
    );
  }

  // --- NEW STEP: THE BATCH PROCESSOR ---
  if (currentStep === 'batch-processing') {
    if (!masterRules || !backdrop) {
      setCurrentStep('positioning'); // Failsafe
      return null;
    }
    
    const subjectsForBatch = processedImages.backgroundRemoved.length > 0 
      ? processedImages.backgroundRemoved
      : workflowFiles;
      
    return (
      <BatchProcessingStep
        subjects={subjectsForBatch}
        backdrop={backdrop}
        masterRules={masterRules}
        isPreCut={processedImages.backgroundRemoved.length === 0}
        onComplete={handleBatchProcessingComplete}
        onBack={() => setCurrentStep('positioning')}
      />
    );
  }

  if (currentStep === 'complete') {
    const finalResults = processedImages.finalComposited?.map(result => ({
      name: result.name,
      finalizedData: result.compositedData
    })) || [];
    
    const transparentImagesForLibrary = processedImages.backgroundRemoved.map(img => ({
      name: img.name,
      data: img.backgroundRemovedData
    }));
    
    return (
      <GalleryPreview
        results={finalResults}
        onBack={onBack}
        title="Batch Processing Complete!"
        transparentImages={transparentImagesForLibrary}
        aiEnhancedImages={[]}
      />
    );
  }

  return null;
};
Phase 3: Fix Preview Component Bugs (Aspect Ratio & Floating)
This fixes Bugs 1 & 2 in the preview UI.

File: src/components/BackdropPositioning.tsx

TypeScript

import React, { useState, useRef, useEffect } from "react";
import { Button } from "@/components/ui/button";
// ... (all other imports) ...
import { cn } from "@/lib/utils";

// ... (interfaces and helpers) ...

export const BackdropPositioning: React.FC<BackdropPositioningProps> = ({
  allSubjects,
  isPreCut,
  onPositioningComplete,
  onBack,
}) => {
  // ... (all existing state hooks) ...
  
  // This is the new, combined function to handle backdrop analysis
  const analyzeAndSetBackdrop = async (file: File, fileUrl: string, source: 'upload' | 'library') => {
    setBackdrop(fileUrl);
    setBackdropFile(file);
    setIsBackdropAnalyzing(true);
    setAiFloorY(null);
    
    try {
      const formData = new FormData();
      formData.append('image', file);
      // This API call now exists
      const { floorY } = await api.analyzeBackdrop(formData); 
      
      setAiFloorY(floorY);
      setPlacement(prev => ({ ...prev, y: floorY })); // AI Snap
      toast({
        title: source === 'upload' ? "AI Floor Detection" : "Backdrop Selected",
        description: `Floor snapped to ${Math.round(floorY * 100)}%`,
      });

    } catch (error) {
      console.error('Error analyzing backdrop:', error);
      toast({
        title: "AI Analysis Failed",
        description: "Defaulting to 75%. You can position it manually.",
        variant: "destructive"
      });
      setAiFloorY(0.75);
      setPlacement(prev => ({ ...prev, y: 0.75 })); // Default Snap
    } finally {
      setIsBackdropAnalyzing(false);
    }
  };

  const handleBackdropUpload = async (event: React.ChangeEvent<HTMLInputElement>) => {
    const file = event.target.files?.[0];
    if (file) {
      const dataUrl = await fileToDataUrl(file);
      await analyzeAndSetBackdrop(file, dataUrl, 'upload');
    }
  };

  const handleLibrarySelect = async (backdrop: any, imageUrl: string) => {
    try {
      const response = await fetch(imageUrl);
      const blob = await response.blob();
      const file = new File([blob], backdrop.name, { type: blob.type });
      await analyzeAndSetBackdrop(file, imageUrl, 'library');
    } catch (error) {
       console.error('Error selecting library backdrop:', error);
       toast({ title: "Error loading library file", variant: "destructive" });
    }
  };

  // --- Start CSS Preview Logic ---
  const getPreviewStyles = () => {
    if (!previewCutout) return {};
    
    const padding = masterPadding / 100;
    const subjectWidthPercent = 100 * (1 - padding * 2);

    // --- üêû BUG 2 FIX: Aspect Ratio ---
    let aspectRatio = '4 / 3'; // Default
    if (masterAspectRatio === '1:1') aspectRatio = '1 / 1';
    if (masterAspectRatio === '3:4') aspectRatio = '3 / 4';
    if (masterAspectRatio === 'original') {
      if (subjectDimensions.w > 1 && subjectDimensions.h > 1) {
        // Calculate aspect ratio based on subject + padding
        const paddedW = subjectDimensions.w / (1 - padding * 2);
        const paddedH = subjectDimensions.h / (1 - padding * 2);
        aspectRatio = `${paddedW} / ${paddedH}`;
      }
    }
    
    return {
      backdropStyles: {
        backgroundImage: `url(${backdrop})`,
        backgroundSize: 'cover',
        backgroundPosition: 'center',
        backgroundRepeat: 'no-repeat',
        aspectRatio: aspectRatio, // Apply the dynamic aspect ratio
      },
      subjectStyles: {
        width: `${subjectWidthPercent}%`,
        height: 'auto',
        left: `50%`,
        top: `${placement.y * 100}%`,
        // --- üêû BUG 1 FIX: Floating Item ---
        // This aligns the *bottom* of the item to the "floor"
        transform: 'translate(-50%, -100%)', 
      }
    };
  };
  
  const { backdropStyles, subjectStyles } = getPreviewStyles();
  // --- End CSS Preview Logic ---

  // ... (rest of the component's JSX) ...
  // Find the main preview <div> and apply the new styles
  
  // In the JSX, find this div (around line 309):
  <div
    className="relative w-full max-w-full overflow-hidden rounded-lg border-2 border-primary/50" // Remove aspect-[4/3]
    style={backdropStyles} // <-- This now contains the aspect ratio
    onMouseDown={(e) => {
      setIsDragging(true);
    }}
    onMouseMove={(e) => {
      if (!isDragging) return;
      const rect = e.currentTarget.getBoundingClientRect();
      // Only update Y, keep X centered
      setPlacement(prev => ({
        ...prev,
        x: 0.5, 
        y: Math.max(0, Math.min(1, (e.clientY - rect.top) / rect.height))
      }));
    }}
    // ... (rest of the div) ...
  >
    {/* ... (Main Subject div) ... */}
  </div>
};
Phase 4: Fix Final Compositing (Cropping & Reflection)
This is the definitive fix for Bug 3 (Cropping) and Bug 4 (Reflection). We replace the broken compositeLayers function.

File: src/lib/canvas-utils.ts

TypeScript

/**
 * Canvas utilities for AI commercial photo editing workflow
 */

export interface SubjectPlacement {
  x: number; // fraction of canvas width (0-1) - typically 0.5
  y: number; // fraction of canvas height (0-1) - the "floor"
  scale: number; // Legacy, no longer used
}

/**
 * Utility to get image dimensions from data URL or blob URL
 */
export const getImageDimensions = (dataUrl: string): Promise<{ width: number; height: number }> => {
  return new Promise((resolve, reject) => {
    const img = new Image();
    img.crossOrigin = "anonymous";
    img.onload = () => {
      resolve({ width: img.naturalWidth, height: img.naturalHeight });
    };
    img.onerror = (err) => reject(new Error(`Failed to load image for dimensions: ${err}`));
    img.src = dataUrl;
  });
};

/**
 * Convert File to data URL
 */
export const fileToDataUrl = (file: File): Promise<string> => {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    reader.onload = (e) => {
      if (e.target?.result) {
        resolve(e.target.result as string);
      } else {
        reject('Failed to read file');
      }
    };
    reader.onerror = reject;
    reader.readAsDataURL(file);
  });
};

/**
 * Helper function to load an image
 */
const loadImage = (src: string, name: string): Promise<HTMLImageElement> => {
  return new Promise((resolve, reject) => {
    const img = new Image();
    img.crossOrigin = "anonymous";
    img.onload = () => resolve(img);
    img.onerror = (err) => reject(new Error(`Failed to load ${name} image: ${err}`));
    img.src = src;
  });
};

/**
 * [FINAL, CORRECTED] Composite backdrop, subject, and reflection
 * This function implements Subject-Centric Cropping and fixes the reflection bug.
 */
export const compositeLayers = async (
  backdropUrl: string,
  subjectWithShadowUrl: string,
  cleanSubjectUrl: string,
  placement: SubjectPlacement, // Contains the X/Y *floor* placement
  outputCanvasSize: { width: number; height: number },
  padding: number // Master padding (e.g., 20)
): Promise<string> => {
  
  console.log('üé® COMPOSITING: Starting Subject-Centric composite');
  
  try {
    const [backdrop, subjectWithShadow, cleanSubject] = await Promise.all([
      loadImage(backdropUrl, 'backdrop'),
      loadImage(subjectWithShadowUrl, 'subject with shadow'),
      loadImage(cleanSubjectUrl, 'clean subject')
    ]);

    const canvas = document.createElement('canvas');
    canvas.width = outputCanvasSize.width;
    canvas.height = outputCanvasSize.height;
    const ctx = canvas.getContext('2d');

    if (!ctx) {
      throw new Error('Failed to get canvas context');
    }

    // --- 1. Draw Backdrop (Cropped/Zoomed) ---
    const canvasAspect = canvas.width / canvas.height;
    const backdropAspect = backdrop.width / backdrop.height;
    
    let scale = 1;
    let drawX = 0;
    let drawY = 0;
    
    // This logic mimics CSS "background-size: cover"
    if (backdropAspect > canvasAspect) { // Backdrop is wider than canvas
      scale = canvas.height / backdrop.height;
      drawX = -(backdrop.width * scale - canvas.width) / 2; // Center X
    } else { // Backdrop is taller than canvas
      scale = canvas.width / backdrop.width;
      drawY = -(backdrop.height * scale - canvas.height) / 2; // Center Y
    }

    let finalDrawX = drawX;
    let finalDrawY = drawY;

    // Adjust Y based on the floor snap
    const scaledBackdropHeight = backdrop.height * scale;
    const backdropFloorPx = (backdrop.height * placement.y) * scale;
    const canvasFloorPx = canvas.height * placement.y;
    
    const yOffset = canvasFloorPx - (backdropFloorPx + finalDrawY);
    finalDrawY = Math.min(0, Math.max(canvas.height - scaledBackdropHeight, finalDrawY + yOffset));
    
    ctx.drawImage(backdrop, finalDrawX, finalDrawY, backdrop.width * scale, backdrop.height * scale);

    // --- 2. Calculate Subject Position (based on padding) ---
    const paddingPercent = padding / 100;
    const subjectAspectRatio = subjectWithShadow.naturalWidth / subjectWithShadow.naturalHeight;

    // This is the "box" the subject must fit in
    const innerBoxW = canvas.width * (1 - paddingPercent * 2);
    const innerBoxH = canvas.height * (1 - paddingPercent * 2);
    
    let scaledWidth = innerBoxW;
    let scaledHeight = scaledWidth / subjectAspectRatio;
    
    if (scaledHeight > innerBoxH) {
      scaledHeight = innerBoxH;
      scaledWidth = scaledHeight * subjectAspectRatio;
    }
    
    const dx = (canvas.width - scaledWidth) / 2; // Center horizontally
    const canvasFloorY = canvas.height * placement.y; // The "floor" line
    const dy = canvasFloorY - scaledHeight; // Position subject's bottom on the floor
    
    // --- 3. Generate and Draw Reflection (THE FIX) ---
    const reflectionHeight = scaledHeight * 0.6; // 60% height
    
    // Create a temporary canvas for the reflection *only*
    const reflectCanvas = document.createElement('canvas');
    reflectCanvas.width = scaledWidth;
    reflectCanvas.height = reflectionHeight;
    const reflectCtx = reflectCanvas.getContext('2d');
    
    if (reflectCtx) {
      reflectCtx.save();
      reflectCtx.scale(1, -1); // Flip vertically
      // Draw the *clean* subject (flipped)
      reflectCtx.drawImage(cleanSubject, 0, -reflectionHeight, scaledWidth, reflectionHeight);
      reflectCtx.restore();
      
      // Apply fade gradient
      const gradient = reflectCtx.createLinearGradient(0, 0, 0, reflectionHeight);
      gradient.addColorStop(0, 'rgba(0, 0, 0, 0.4)'); // Start at 40% opacity
      gradient.addColorStop(0.6, 'rgba(0, 0, 0, 0.1)');
      gradient.addColorStop(1, 'rgba(0, 0, 0, 0)');
      
      reflectCtx.globalCompositeOperation = 'destination-in';
      reflectCtx.fillStyle = gradient;
      reflectCtx.fillRect(0, 0, scaledWidth, reflectionHeight);
      
      // Draw the reflection from the temp canvas to the main canvas
      ctx.save();
      ctx.filter = 'blur(2px)';
      ctx.drawImage(reflectCanvas, dx, canvasFloorY); // Position reflection *at* the floor line
      ctx.restore();
    }
    
    // --- 4. Draw Subject (with shadow) ---
    ctx.drawImage(subjectWithShadow, dx, dy, scaledWidth, scaledHeight);

    const finalDataUrl = canvas.toDataURL('image/png');
    console.log('Compositing complete.');
    
    return finalDataUrl;

  } catch (error) {
    console.error('Error during compositing:', error);
    throw error;
  }
};


// ... (other functions: convertBlackToTransparent, applyMaskToImage, etc.) ...